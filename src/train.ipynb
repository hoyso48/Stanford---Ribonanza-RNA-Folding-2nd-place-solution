{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34534520-c085-47ed-90c2-d6dc4afa49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pandas neptune transformers matplotlib tqdm scikit-learn scipy opencv-python-headless torch_optimizer torchinfo pyarrow fastai accelerate\n",
    "# !sudo apt install build-essential --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89121256-74f4-4690-8e9a-bcbae93e1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCESSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91545a7e-5af1-4adc-91c2-1cdcd6cf7dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoyso/miniconda3/envs/hoyso_ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.model import RNARegModel\n",
    "from datasets.dataset import RNADataset, collate_fn\n",
    "from utils.util import seed_everything, clear_everything\n",
    "from callbacks.callback import SaveLastCallback\n",
    "from losses.loss import mae_loss, weighted_mae_loss\n",
    "from metrics.metric import MAE, MAE_ave\n",
    "\n",
    "from fastai.vision.all import GradientClip, DataLoaders, DataLoader, Metric, Learner, Adam, CSVLogger\n",
    "from fastai.distributed import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.autonotebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd99357-74f5-488f-95c8-3e1c06d20de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 2023\n",
    "    n_splits = 5\n",
    "    fold = 0\n",
    "    use_all_train = False #use all train data or not, fold will be ignored when True.\n",
    "    max_len = -1 #input length will never exceed this value, -1 for unbounded. set it to 206 when num_processes>1\n",
    "    dynamic_len = True #apply dynamic padding on batch or not. If False, input length will be fixed to max_len(max_len must be > 0 for this case). set it to False when num_processes>1\n",
    "    \n",
    "    mixed_precision = True\n",
    "\n",
    "    num_workers = 8\n",
    "    device = 'cuda'\n",
    "    # grad_accumulation = 1\n",
    "    clip_grad = 3.0\n",
    "    lr = 2e-3 ##for stability. 4e-3 is slightly better in scores.\n",
    "    weight_decay = 0.01\n",
    "    epoch = 60 \n",
    "    warmup = 0.01\n",
    "    train_batch_size = 256\n",
    "    valid_batch_size = train_batch_size\n",
    "    valid_drop_last = False #set it to True when num_processes>1\n",
    "    train_signal_to_noise_filter = 1.0 #signal_to_noise>this value\n",
    "\n",
    "    dim = 192\n",
    "    num_layers = 12\n",
    "    num_heads = 4\n",
    "    \n",
    "    root_dir = '.'\n",
    "    data_dir = '../datamount'\n",
    "    output_dir = '../outputs'\n",
    "    model_dir = 'model_ckpts'\n",
    "    model_name = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b389d3d-c127-40ab-a5f7-0244b3662fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg=CFG):\n",
    "    os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "    os.chdir(cfg.root_dir)\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "    os.makedirs(f'{cfg.output_dir}/{cfg.model_dir}', exist_ok=True)\n",
    "    seed_everything(cfg.seed)\n",
    "    train_df = pd.read_parquet(f'{cfg.data_dir}/train_data_new.parquet')#.sample(frac=0.01).reset_index(drop=True)\n",
    "\n",
    "    df = train_df\n",
    "    if cfg.use_all_train:\n",
    "        train = df\n",
    "        valid = df[:0]\n",
    "    else:\n",
    "        df['fold'] = -1\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "        kfold = KFold(n_splits=cfg.n_splits, random_state=cfg.seed, shuffle=True)\n",
    "        for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(df)):\n",
    "            df.loc[valid_idx, 'fold'] = fold_idx\n",
    "    \n",
    "        assert not (df['fold']==-1).sum()\n",
    "        assert len(np.unique(df['fold']))==cfg.n_splits\n",
    "    \n",
    "        train = df.loc[df['fold']!=cfg.fold]\n",
    "        valid = df.loc[df['fold']==cfg.fold]\n",
    "\n",
    "    train_ds = RNADataset(train, mode='train', SN_filter=False, signal_to_noise_filter=cfg.train_signal_to_noise_filter, dir=cfg.data_dir)\n",
    "    valid_ds = RNADataset(valid, mode='valid', SN_filter=True, dir=cfg.data_dir)\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                        batch_size=cfg.train_batch_size,\n",
    "                        shuffle=True,\n",
    "                        sampler=None,\n",
    "                        num_workers=cfg.num_workers,\n",
    "                        persistent_workers=True,\n",
    "                        drop_last=True,\n",
    "                        pin_memory=True,\n",
    "                        prefetch_factor=2,\n",
    "                        create_batch=lambda x: collate_fn(x, cfg.max_len, cfg.dynamic_len),\n",
    "                       )\n",
    "    valid_loader = DataLoader(valid_ds,\n",
    "                        batch_size=cfg.valid_batch_size,\n",
    "                        shuffle=False,\n",
    "                        sampler=None,\n",
    "                        num_workers=cfg.num_workers,\n",
    "                        persistent_workers=True,\n",
    "                        drop_last=cfg.valid_drop_last,\n",
    "                        pin_memory=True,\n",
    "                        prefetch_factor=2,\n",
    "                        create_batch=lambda x: collate_fn(x, cfg.max_len, cfg.dynamic_len),\n",
    "                       )\n",
    "    model = RNARegModel(dim=cfg.dim, num_layers=cfg.num_layers, num_heads=cfg.num_heads)\n",
    "    model = torch.compile(model.to(cfg.device))\n",
    "    data = DataLoaders(train_loader, valid_loader)\n",
    "    clear_everything()\n",
    "\n",
    "    learn = Learner(data, \n",
    "                model, \n",
    "                loss_func=weighted_mae_loss,\n",
    "                path=cfg.output_dir,\n",
    "                model_dir=cfg.model_dir,\n",
    "                cbs=[GradientClip(cfg.clip_grad), SaveLastCallback(cfg.model_name), CSVLogger(fname=f'{cfg.model_name}-log.csv')],\n",
    "                opt_func=partial(Adam, lr=cfg.lr, mom=0.9, sqr_mom=0.999, eps=1e-8),\n",
    "            metrics=[MAE_ave()]).to_fp16(enabled=cfg.mixed_precision)\n",
    "\n",
    "    with learn.distrib_ctx(in_notebook=True, sync_bn=False): #masked sync_bn is not implemented\n",
    "\n",
    "        learn.fit_one_cycle(cfg.epoch,\n",
    "                            lr_max=cfg.lr,\n",
    "                            wd=cfg.weight_decay,\n",
    "                            pct_start=cfg.warmup)\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733e7dcb-23c5-4f7d-8a33-2eb8135312b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/hoyso/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "Launching training on one GPU.\n",
      "Training Learner...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae_ave</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.199442</td>\n",
       "      <td>0.192536</td>\n",
       "      <td>0.145276</td>\n",
       "      <td>03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.185810</td>\n",
       "      <td>0.181758</td>\n",
       "      <td>0.138857</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.181360</td>\n",
       "      <td>0.178604</td>\n",
       "      <td>0.136511</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177917</td>\n",
       "      <td>0.176190</td>\n",
       "      <td>0.134855</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.176790</td>\n",
       "      <td>0.173685</td>\n",
       "      <td>0.133273</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>0.170592</td>\n",
       "      <td>0.131440</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.171243</td>\n",
       "      <td>0.171730</td>\n",
       "      <td>0.132104</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.169372</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.129873</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.168517</td>\n",
       "      <td>0.167031</td>\n",
       "      <td>0.129308</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.167307</td>\n",
       "      <td>0.166713</td>\n",
       "      <td>0.129103</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.166455</td>\n",
       "      <td>0.165379</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.164827</td>\n",
       "      <td>0.164484</td>\n",
       "      <td>0.127622</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.163963</td>\n",
       "      <td>0.163284</td>\n",
       "      <td>0.127014</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.162456</td>\n",
       "      <td>0.163106</td>\n",
       "      <td>0.126792</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.161996</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.126375</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.161272</td>\n",
       "      <td>0.161270</td>\n",
       "      <td>0.125763</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.160573</td>\n",
       "      <td>0.160583</td>\n",
       "      <td>0.125234</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.159196</td>\n",
       "      <td>0.160843</td>\n",
       "      <td>0.125571</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.159247</td>\n",
       "      <td>0.160357</td>\n",
       "      <td>0.125197</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.157927</td>\n",
       "      <td>0.159392</td>\n",
       "      <td>0.124584</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.156878</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>0.125311</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.156566</td>\n",
       "      <td>0.158703</td>\n",
       "      <td>0.124159</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.155766</td>\n",
       "      <td>0.158593</td>\n",
       "      <td>0.124154</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.155103</td>\n",
       "      <td>0.159191</td>\n",
       "      <td>0.124535</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.154430</td>\n",
       "      <td>0.157885</td>\n",
       "      <td>0.123648</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.153470</td>\n",
       "      <td>0.157508</td>\n",
       "      <td>0.123442</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.153194</td>\n",
       "      <td>0.157857</td>\n",
       "      <td>0.123708</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.152724</td>\n",
       "      <td>0.157168</td>\n",
       "      <td>0.123377</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.152164</td>\n",
       "      <td>0.156560</td>\n",
       "      <td>0.122868</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.150677</td>\n",
       "      <td>0.156436</td>\n",
       "      <td>0.122812</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>0.156301</td>\n",
       "      <td>0.122835</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.149780</td>\n",
       "      <td>0.155934</td>\n",
       "      <td>0.122519</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.149564</td>\n",
       "      <td>0.155986</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.148462</td>\n",
       "      <td>0.155448</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.147981</td>\n",
       "      <td>0.155645</td>\n",
       "      <td>0.122380</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.147410</td>\n",
       "      <td>0.155192</td>\n",
       "      <td>0.122079</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.146568</td>\n",
       "      <td>0.155346</td>\n",
       "      <td>0.122233</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.146816</td>\n",
       "      <td>0.154941</td>\n",
       "      <td>0.121928</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.146083</td>\n",
       "      <td>0.154524</td>\n",
       "      <td>0.121682</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.145459</td>\n",
       "      <td>0.154486</td>\n",
       "      <td>0.121669</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.144804</td>\n",
       "      <td>0.154403</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.144730</td>\n",
       "      <td>0.154653</td>\n",
       "      <td>0.121809</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.144021</td>\n",
       "      <td>0.154329</td>\n",
       "      <td>0.121630</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.143729</td>\n",
       "      <td>0.154211</td>\n",
       "      <td>0.121552</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.143708</td>\n",
       "      <td>0.154197</td>\n",
       "      <td>0.121545</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.142650</td>\n",
       "      <td>0.154166</td>\n",
       "      <td>0.121532</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.142183</td>\n",
       "      <td>0.153818</td>\n",
       "      <td>0.121294</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.142303</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.141935</td>\n",
       "      <td>0.153895</td>\n",
       "      <td>0.121383</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.141449</td>\n",
       "      <td>0.153839</td>\n",
       "      <td>0.121353</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.141103</td>\n",
       "      <td>0.153770</td>\n",
       "      <td>0.121305</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.141130</td>\n",
       "      <td>0.153752</td>\n",
       "      <td>0.121290</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.140585</td>\n",
       "      <td>0.153659</td>\n",
       "      <td>0.121232</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.140674</td>\n",
       "      <td>0.153709</td>\n",
       "      <td>0.121260</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.140440</td>\n",
       "      <td>0.153605</td>\n",
       "      <td>0.121205</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.140934</td>\n",
       "      <td>0.153693</td>\n",
       "      <td>0.121271</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.140424</td>\n",
       "      <td>0.153603</td>\n",
       "      <td>0.121208</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.140714</td>\n",
       "      <td>0.153634</td>\n",
       "      <td>0.121234</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.140499</td>\n",
       "      <td>0.153636</td>\n",
       "      <td>0.121234</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.153621</td>\n",
       "      <td>0.121222</td>\n",
       "      <td>02:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoyso/miniconda3/envs/hoyso_ml/lib/python3.9/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/hoyso/miniconda3/envs/hoyso_ml/lib/python3.9/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/hoyso/miniconda3/envs/hoyso_ml/lib/python3.9/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/hoyso/miniconda3/envs/hoyso_ml/lib/python3.9/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()\n",
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(train, num_processes=NUM_PROCESSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87398-f4a3-4664-abcd-326bf50ce1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
